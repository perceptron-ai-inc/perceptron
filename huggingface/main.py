"""
Demonstration of HuggingFace Perceptron pipeline using modular implementation.

This script shows how to:
1. Create a multimodal Document with text and images
2. Process it through IsaacProcessor to create TensorStreams
3. Run the TensorStream through IsaacForCausalLM model
4. Generate text continuations

Example usage:
    python -m huggingface.main
"""

from __future__ import annotations

import torch
from PIL import Image as PILImage
from PIL import ImageDraw, ImageFont
import base64
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM
from loguru import logger

# Prefer local repo package over any site-installed "perceptron"
import os
import sys
REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if REPO_ROOT not in sys.path:
    sys.path.insert(0, REPO_ROOT)

from perceptron.tensorstream import VisionType
from perceptron.tensorstream.ops import tensor_stream_token_view, modality_mask
from huggingface.modular_isaac import IsaacProcessor
from perceptron.pointing.parser import extract_points


# Create a dummy multimodal input (text + image) without external schema dependencies
# Each item is a dict with keys: type in {"text","image"}, content (string), role (optional)
DUMMY_DOCUMENT = [
    {
        "type": "text",
        "content": "<hint>BOX</hint>",
        "role": "user",
    },
    {
        "type": "image",
        "content": "huggingface/assets/example.webp",
        "role": "user",
    },
    {
        "type": "text",
        "content": "Determine whether it is safe to cross the street. Look for signage and moving traffic.",
        "role": "user",
    },
]


def document_to_messages(
    document: list[dict], vision_token: str = "<image>"
) -> tuple[list[dict[str, str]], list[PILImage.Image]]:
    """
    Convert a Document to messages format compatible with chat templates.
    Each content turn creates its own message entry.

    Args:
        document: list of dicts containing Text and/or Image content
        vision_token: Token to use for image placeholder

    Returns:
        Tuple of (messages, images) where messages is a list of dicts with 'role' and 'content'
    """
    messages = []
    images = []

    for item in document:
        itype = item.get("type")
        if itype == "text":
            content = item.get("content")
            if content:
                messages.append(
                    {
                        "role": item.get("role", "user"),
                        "content": content,
                    }
                )
        elif itype == "image":
            content = item.get("content")
            if content:
                img = PILImage.open(content)
                images.append(img)
                messages.append(
                    {
                        "role": item.get("role", "user"),
                        "content": vision_token,
                    }
                )

    return messages, images


def decode_tensor_stream(tensor_stream, tokenizer):
    """Decode a TensorStream to see its text content."""
    token_view = tensor_stream_token_view(tensor_stream)
    mod = modality_mask(tensor_stream)

    # Get text tokens (excluding vision tokens)
    text_tokens = token_view[(mod != VisionType.image.value)]
    decoded = tokenizer.decode(text_tokens[0] if len(text_tokens.shape) > 1 else text_tokens)

    return decoded


def visualize_predictions(generated_text: str, image: PILImage.Image, output_path: str = "prediction.jpeg"):
    """Extract bounding boxes from generated text and render them on the input image.

    Args:
        generated_text: The text generated by the model containing point_box tags
        image: The input PIL image to draw boxes on
        output_path: Path to save the resulting image with visualized boxes
    """
    # Extract bounding boxes from the generated text
    boxes = extract_points(generated_text, expected="box")

    if not boxes:
        logger.info("No bounding boxes found in the generated text")
        image.save(output_path)
        return

    # Get image dimensions
    img_width, img_height = image.size
    logger.info(f"Image dimensions: {img_width}x{img_height}")

    # Create a copy of the image to draw on
    img_with_boxes = image.copy()
    draw = ImageDraw.Draw(img_with_boxes)

    # Try to use a basic font, fall back to default if not available
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 16)
    except:
        font = ImageFont.load_default()

    # Define colors for different boxes
    colors = ["red", "green", "blue", "yellow", "magenta", "cyan", "orange", "purple"]

    logger.info(f"Found {len(boxes)} bounding box(es) to render")

    for idx, box in enumerate(boxes):
        color = colors[idx % len(colors)]

        # Extract normalized coordinates (0-1000 range)
        norm_x1, norm_y1 = box.top_left.x, box.top_left.y
        norm_x2, norm_y2 = box.bottom_right.x, box.bottom_right.y

        # Scale coordinates from 0-1000 range to actual image dimensions
        x1 = int((norm_x1 / 1000.0) * img_width)
        y1 = int((norm_y1 / 1000.0) * img_height)
        x2 = int((norm_x2 / 1000.0) * img_width)
        y2 = int((norm_y2 / 1000.0) * img_height)

        # Ensure coordinates are within image bounds
        x1 = max(0, min(x1, img_width - 1))
        y1 = max(0, min(y1, img_height - 1))
        x2 = max(0, min(x2, img_width - 1))
        y2 = max(0, min(y2, img_height - 1))

        # Draw the bounding box
        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)

        # Add label if mention exists
        if box.mention:
            # Calculate text position (above the box if possible)
            text_y = max(y1 - 20, 5)

            # Draw text background for better visibility
            text_bbox = draw.textbbox((x1, text_y), box.mention, font=font)
            draw.rectangle(text_bbox, fill=color)
            draw.text((x1, text_y), box.mention, fill="white", font=font)

            logger.info(f"Box {idx+1}: {box.mention} - normalized: ({norm_x1},{norm_y1}) to ({norm_x2},{norm_y2}) -> scaled: ({x1},{y1}) to ({x2},{y2})")
        else:
            logger.info(f"Box {idx+1}: normalized: ({norm_x1},{norm_y1}) to ({norm_x2},{norm_y2}) -> scaled: ({x1},{y1}) to ({x2},{y2})")

    # Save the image with bounding boxes
    img_with_boxes.save(output_path, "JPEG")
    logger.info(f"Saved visualization to {output_path}")


def main():
    """Main demonstration of the HuggingFace Perceptron pipeline."""
    logger.info("=" * 60)
    logger.info("HuggingFace Perceptron Modular Implementation Demo")
    logger.info("=" * 60)

    hf_path = "PerceptronAI/Isaac-0.1"

    # Load processor and config from the HF checkpoint
    logger.info(f"Loading processor and config from HF checkpoint: {hf_path}")
    tokenizer = AutoTokenizer.from_pretrained(hf_path, trust_remote_code=True, use_fast=False)
    config = AutoConfig.from_pretrained(hf_path, trust_remote_code=True)
    processor = IsaacProcessor(tokenizer=tokenizer, config=config)

    # Load model from the HF checkpoint using AutoModelForCausalLM
    logger.info(f"Loading AutoModelForCausalLM from HF checkpoint: {hf_path}")
    model = AutoModelForCausalLM.from_pretrained(hf_path, trust_remote_code=True)

    # Move to appropriate device and dtype
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    model = model.to(device=device, dtype=dtype)
    model.eval()

    logger.info(f"Model loaded on {device} with dtype {dtype}")

# Process the dummy document using chat templates
    logger.info("\nProcessing dummy document:")
    logger.info(f"Document content: {DUMMY_DOCUMENT}")

    # Convert document to messages format
    messages, images = document_to_messages(DUMMY_DOCUMENT, vision_token=config.vision_token)
    logger.info(f"\nConverted to messages: {messages}")
    logger.info(f"Number of images: {len(images)}")

    # Apply chat template to get formatted text
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    logger.info(f"\nFormatted text after chat template:\n{text}")

    # Process with IsaacProcessor
    device = next(model.parameters()).device
    inputs = processor(text=text, images=images, return_tensors="pt")
    tensor_stream = inputs["tensor_stream"].to(device)
    input_ids = inputs["input_ids"].to(device)

    # Decode and display the processed content
    decoded_content = decode_tensor_stream(tensor_stream, processor.tokenizer)
    logger.info(f"\nProcessed content (decoded):\n{decoded_content}")

    # Generate text using the model
    logger.info("\nGenerating text using the model...")

    with torch.no_grad():
        # Generate with TensorStream (multimodal generation)
        logger.info("\nMultimodal generation with TensorStream and chat template:")
        generated_ids = model.generate(
            tensor_stream=tensor_stream,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=processor.tokenizer.eos_token_id,
            eos_token_id=processor.tokenizer.eos_token_id,
        )

        # Decode the generated text
        logger.info(f"\nInput shape: {tensor_stream.shape}")
        logger.info(f"Generated shape: {generated_ids.shape}")

        # Decode full output
        generated_text = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=False)
        logger.info(f"\nFull generated output:\n{generated_text}")

        # Also show just the new tokens (excluding the input)
        if generated_ids.shape[1] > input_ids.shape[1]:
            new_tokens = generated_ids[0, input_ids.shape[1] :]
            new_text = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)
            logger.info(f"\nNew tokens only: '{new_text}' ({len(new_tokens)} tokens)")

        # Visualize predictions with bounding boxes
        logger.info("\nVisualizing predictions with bounding boxes...")
        if images and len(images) > 0:
            # Use the first image for visualization
            visualize_predictions(generated_text, images[0], output_path="prediction.jpeg")
        else:
            logger.warning("No input image available for visualization")


if __name__ == "__main__":
    main()
